{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import `pyswip` and consult the Prolog background knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyswip import Prolog\n",
    "prolog = Prolog()\n",
    "prolog.consult('mnist_sum.pl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test if the `CLP(FD)`-based abduction in the background knowledge base works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 adds 9 equals 9.\n",
      "1 adds 8 equals 9.\n",
      "2 adds 7 equals 9.\n",
      "3 adds 6 equals 9.\n",
      "4 adds 5 equals 9.\n",
      "5 adds 4 equals 9.\n",
      "6 adds 3 equals 9.\n",
      "7 adds 2 equals 9.\n",
      "8 adds 1 equals 9.\n",
      "9 adds 0 equals 9.\n"
     ]
    }
   ],
   "source": [
    "target = 9\n",
    "for soln in prolog.query(\"abduce([X,Y], {})\".format(target)):\n",
    "    print(soln[\"X\"], \"adds\", soln[\"Y\"], \"equals {}.\".format(target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abductive Learning\n",
    "\n",
    "Now let's try to implement the MNIST sum learning algorithm using the Abductive Learning framework.\n",
    "\n",
    "### Dataset Generation\n",
    "\n",
    "Directly copy the codes from the `data_generator.ipynb` notebook file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "dataset1 = datasets.MNIST('data', train=True, download=True,\n",
    "                            transform=transform)\n",
    "dataset2 = datasets.MNIST('data', train=False,\n",
    "                            transform=transform)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "digit_groups_train = {0:[], 1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[], 8:[], 9:[]}\n",
    "digit_groups_test = {0:[], 1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[], 8:[], 9:[]}\n",
    "\n",
    "for i in range(len(dataset1)): \n",
    "    digit_groups_train[int(dataset1.targets[i])].append(i)\n",
    "for i in range(len(dataset2)): \n",
    "    digit_groups_test[int(dataset2.targets[i])].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Sum:\n",
    "    def __init__(self, num, digit_groups):\n",
    "        self.targets = []\n",
    "        self.img_indices = []\n",
    "        self.ground_truth = []\n",
    "        self.length = num\n",
    "        for i in range(num):\n",
    "            # sampling two numbers from 0 to 9\n",
    "            sampled_digits = np.random.choice(10, 2)\n",
    "            self.ground_truth.append(list(sampled_digits))\n",
    "\n",
    "            # using the sum of the sampled digits as the target\n",
    "            self.targets.append(sum(sampled_digits))\n",
    "            ids = []\n",
    "            for j in range(len(sampled_digits)):\n",
    "                # get the j-th digits\n",
    "                digit = sampled_digits[j]\n",
    "                # total number of the images of the digit\n",
    "                ids.append(np.random.choice(digit_groups[digit]))\n",
    "            self.img_indices.append(ids)\n",
    "\n",
    "# Generate the training and test dataset for MNIST Sum task\n",
    "mnist_sum_data_train = MNIST_Sum(3000, digit_groups_train)\n",
    "mnist_sum_data_test = MNIST_Sum(3000, digit_groups_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Machine Learning Part\n",
    "\n",
    "Neural networks for image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "\n",
    "def conv_net(outdim, *args, **kwargs):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(1, 32, 3, 1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 64, 3, 1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.Dropout(0.25),\n",
    "        Flatten(),\n",
    "        nn.Linear(9216, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(128, outdim),\n",
    "        nn.LogSoftmax(dim=1)\n",
    "    )\n",
    "\n",
    "\n",
    "def auto_enc(outdim, *args, **kwargs):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(outdim, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 784)\n",
    "    )\n",
    "\n",
    "\n",
    "def mlp(indim, outdim, *args, **kwargs):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(indim, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, outdim),\n",
    "        nn.LogSoftmax(dim=1),\n",
    "    )\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    \"\"\"A (Bi)LSTM Model.\n",
    "\n",
    "    Attributes:\n",
    "        num_layers: the number of LSTM layers (number of stacked LSTM models) in the network.\n",
    "        in_dim: the size of the input sample.\n",
    "        hidden_dim: the size of the hidden layers.\n",
    "        out_dim: the size of the output.\n",
    "        activation: the activation function.\n",
    "        bidirectional: the flag for bidirectional LSTM\n",
    "        dropout: the dropout rate if num_layers > 1\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_layers, in_dim, hidden_dim, out_dim,\n",
    "                 bidirectional=False, dropout=0):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.bidirectional = bidirectional\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.lstm = nn.LSTM(self.in_dim,\n",
    "                            self.hidden_dim,\n",
    "                            num_layers=self.num_layers,\n",
    "                            bidirectional=self.bidirectional,\n",
    "                            dropout=self.dropout,\n",
    "                            batch_first=True)\n",
    "        fc_dim = self.hidden_dim * 2 if self.bidirectional else self.hidden_dim\n",
    "        self.fc = nn.Linear(fc_dim, self.out_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        lstm_out, _ = self.lstm(inputs)\n",
    "        outputs = self.fc(lstm_out[:, -1, :])\n",
    "        outputs = torch.sigmoid(outputs)\n",
    "        return outputs\n",
    "\n",
    "    def loss_function(self, pred, y):\n",
    "        return F.binary_cross_entropy(pred, y.view(y.shape[0], -1))\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    outdim = 10\n",
    "\n",
    "    def __init__(self, outdim):\n",
    "        super(Net, self).__init__()\n",
    "        self.outdim = outdim\n",
    "        self.enc = conv_net(outdim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.enc(x)\n",
    "        return output\n",
    "\n",
    "    def loss_function(self, pred, y):\n",
    "        return F.nll_loss(pred, y)\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch,\n",
    "          log_interval=1000, dry_run=False):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = model.loss_function(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if dry_run:\n",
    "                break\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            # sum up batch loss\n",
    "            test_loss += model.loss_function(output, target).item()\n",
    "            # get the index of the max log-probability\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('-- Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Logic Abduction Part\n",
    "\n",
    "It involves the following steps:\n",
    "1. Given the target (label, i.e., the sum of the two images), using `pyswip` to abduce possible pseudo-labels for them.\n",
    "2. Calculate the probability of each pair of pseudo-labels.\n",
    "3. Return the most probable pseudo-labels to retrain the neural network.\n",
    "\n",
    "_Remark_: For more complicated problems, a better way of searching for the best pseudo-labels is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Abducing possible pseudo-labels given the sum\n",
    "\n",
    "- `pl` is the Prolog instance that consulted `mnist_sum.pl`;\n",
    "- `target` is the sum of two images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abduce(pl, target):\n",
    "    # This abduce/2 function is defined in \"mnist_sum.pl\"\n",
    "    ans = [];\n",
    "    for soln in pl.query(\"abduce([X,Y], {})\".format(target)):\n",
    "        ans.append([soln[\"X\"], soln[\"Y\"]])\n",
    "    if len(ans) > 0:\n",
    "        return ans\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the `abduce` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6, 9], [7, 8], [8, 7], [9, 6]]\n"
     ]
    }
   ],
   "source": [
    "print(abduce(prolog, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Abductive Learning Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing useful libraries and set the default neural network training parameters within the Abductive Learning Process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "nn_train_kwargs = {'batch_size': 64, 'shuffle': True}\n",
    "nn_epoch = 2\n",
    "\n",
    "nn_test_loader = torch.utils.data.DataLoader(dataset2, **nn_train_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful functions for abduction:\n",
    "1. `get_mnist_imgs`: given `indices`, sample a subset of images from `dataset` (such as the `MNIST` dataset).\n",
    "2. `best_pseudo_label`: given a set of abduced possible pseudo-labels and the pseudo-label distribution, return the most probable pseudo-label combination for each image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist_imgs(dataset, indices, use_cuda=False):\n",
    "    \"\"\"\n",
    "    Given get the image tensor from mnist dataset by indices\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(indices)\n",
    "    img_tensor, tgt = dataset[indices[0]]\n",
    "    img_tensor = torch.reshape(img_tensor, (1, 1, 28, 28))\n",
    "    targets = [tgt]\n",
    "    i = 1\n",
    "    while i < n:\n",
    "        img, tgt = dataset[indices[i]]\n",
    "        img = torch.reshape(img, (1, 1, 28, 28))\n",
    "        img_tensor = torch.cat((img_tensor, img), 0)\n",
    "        targets.append(tgt)\n",
    "        i = i + 1\n",
    "    if use_cuda:\n",
    "        img_tensor = img_tensor.to(torch.device(\"cuda\"))\n",
    "    return img_tensor, targets\n",
    "\n",
    "def best_pseudo_label(pseudo_label_lists, pseudo_label_dist):\n",
    "    best_score = -100000.0\n",
    "    best_combi = np.zeros(pseudo_label_dist.shape[0])\n",
    "    probabilities = np.exp(pseudo_label_dist)\n",
    "    for label_combi in pseudo_label_lists:\n",
    "        # because the scores are log_softmax, the log probability can be calculated as sum\n",
    "        score = 1.0\n",
    "        for j in range(len(label_combi)):\n",
    "            score = score*probabilities[j, label_combi[j]]\n",
    "        if score >= best_score:\n",
    "            best_score = score\n",
    "            best_combi = label_combi\n",
    "    return best_combi, score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main procedure for abductive Learning. Given a machine learning `model` and a prolog instance `pl` with `dataset`, it does the following steps:\n",
    "1. Using `model` to predict the pseudo-label probabilistic distribution of `dataset`;\n",
    "2. Finding the best pseudo-label combination considering both the abduction result from `pl` and the pseudo-label probabilistic distribution;\n",
    "3. Retrain the neural network with the abduced pseudo-labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ABL_main(model, pl, dataset, optimizer=None, scheduler=None):\n",
    "    # number of examples\n",
    "    num_examples = dataset.length\n",
    "    abduced_data_ids = []\n",
    "    abduced_labels = []\n",
    "    ground_truth_labels = []\n",
    "\n",
    "    # start abduction\n",
    "    for i in tqdm (range(num_examples), desc=\"Abducing...\"):\n",
    "        target = int(dataset.targets[i])\n",
    "        possible_pseudo_labels = abduce(pl, target)\n",
    "        if possible_pseudo_labels is not None:\n",
    "            # reshape the tensor of the two MNIST images to match NN model's input dimensions\n",
    "            img_indices = dataset.img_indices[i]\n",
    "            imgs, _ = get_mnist_imgs(dataset1, img_indices, use_cuda=False)\n",
    "\n",
    "            pseudo_label_distribution = model(imgs).detach().numpy()\n",
    "\n",
    "            # find the pseudo-labels with the maximum likelihood\n",
    "            abduced_pseudo_labels, _ = best_pseudo_label(possible_pseudo_labels, pseudo_label_distribution)\n",
    "\n",
    "            # for abduced dataset\n",
    "            abduced_data_ids = abduced_data_ids + img_indices\n",
    "            abduced_labels = abduced_labels + abduced_pseudo_labels\n",
    "            ground_truth_labels = ground_truth_labels + dataset.ground_truth[i]\n",
    "\n",
    "    # changing the training data labels to the abduced labels\n",
    "    for i, img in enumerate(abduced_data_ids):\n",
    "        dataset1.targets[img] = abduced_labels[i]\n",
    "    \n",
    "    abduction_accuracy = np.sum(np.array(ground_truth_labels) == np.array(abduced_labels))/len(abduced_labels)\n",
    "\n",
    "\n",
    "    # making new dataset with abduced labels\n",
    "    abduced_data = torch.utils.data.Subset(dataset1, abduced_data_ids)\n",
    "\n",
    "    # training the neural network model\n",
    "    abduced_train_loader = torch.utils.data.DataLoader(abduced_data, batch_size=64)\n",
    "\n",
    "    for epoch in range(1, nn_epoch + 1):\n",
    "        train(model, device, abduced_train_loader, optimizer, epoch)\n",
    "        print(\"Abduction accuracy: \", abduction_accuracy)\n",
    "        scheduler.step()\n",
    "    test(model, device, nn_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise model and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(outdim=10).to(device)\n",
    "\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=1.0)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run Abductive Learning without any pre-train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:03<00:00, 911.94it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 2.273243\n",
      "Abduction accuracy:  0.20433333333333334\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 2.110063\n",
      "Abduction accuracy:  0.20433333333333334\n",
      "-- Test set: Average loss: 0.0342, Accuracy: 2936/10000 (29%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:02<00:00, 1053.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 1.796165\n",
      "Abduction accuracy:  0.376\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 1.533165\n",
      "Abduction accuracy:  0.376\n",
      "-- Test set: Average loss: 0.0333, Accuracy: 4967/10000 (50%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:02<00:00, 1046.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 1.341715\n",
      "Abduction accuracy:  0.454\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 1.339319\n",
      "Abduction accuracy:  0.454\n",
      "-- Test set: Average loss: 0.0339, Accuracy: 5350/10000 (54%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:02<00:00, 1079.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 1.163868\n",
      "Abduction accuracy:  0.5353333333333333\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 1.038307\n",
      "Abduction accuracy:  0.5353333333333333\n",
      "-- Test set: Average loss: 0.0299, Accuracy: 6146/10000 (61%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:02<00:00, 1069.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.972730\n",
      "Abduction accuracy:  0.606\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.988881\n",
      "Abduction accuracy:  0.606\n",
      "-- Test set: Average loss: 0.0280, Accuracy: 6867/10000 (69%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:02<00:00, 1017.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.848105\n",
      "Abduction accuracy:  0.666\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 1.007171\n",
      "Abduction accuracy:  0.666\n",
      "-- Test set: Average loss: 0.0255, Accuracy: 7318/10000 (73%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:02<00:00, 1068.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.860467\n",
      "Abduction accuracy:  0.726\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.748814\n",
      "Abduction accuracy:  0.726\n",
      "-- Test set: Average loss: 0.0238, Accuracy: 7944/10000 (79%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:02<00:00, 1066.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.676006\n",
      "Abduction accuracy:  0.77\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.674968\n",
      "Abduction accuracy:  0.77\n",
      "-- Test set: Average loss: 0.0227, Accuracy: 8307/10000 (83%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:02<00:00, 1070.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.649832\n",
      "Abduction accuracy:  0.7883333333333333\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.618356\n",
      "Abduction accuracy:  0.7883333333333333\n",
      "-- Test set: Average loss: 0.0221, Accuracy: 8375/10000 (84%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:02<00:00, 1059.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.633878\n",
      "Abduction accuracy:  0.793\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.645498\n",
      "Abduction accuracy:  0.793\n",
      "-- Test set: Average loss: 0.0219, Accuracy: 8397/10000 (84%)\n"
     ]
    }
   ],
   "source": [
    "ABL_epochs = 10\n",
    "for epoch in range(ABL_epochs):\n",
    "    ABL_main(model, prolog, mnist_sum_data_train, optimizer=optimizer, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run Abductive Learning with one-shot pre-train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample an one-shot training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/10 (0%)]\tLoss: 2.318949\n",
      "Train Epoch: 2 [0/10 (0%)]\tLoss: 2.680890\n",
      "Train Epoch: 3 [0/10 (0%)]\tLoss: 1.544642\n",
      "Train Epoch: 4 [0/10 (0%)]\tLoss: 1.487406\n",
      "-- Test set: Average loss: 0.0311, Accuracy: 3195/10000 (32%)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# reset the machine learning model\n",
    "model = Net(outdim=10).to(device)\n",
    "\n",
    "# reset dataset1 to reset the labels for one-shot training, \n",
    "# since the previous abductive learning process has changed \n",
    "# the ground truth labels in dataset1\n",
    "dataset1 = datasets.MNIST('data', train=True, download=True,\n",
    "                            transform=transform)\n",
    "\n",
    "n_samples = 1\n",
    "few_shot_indices = []\n",
    "\n",
    "for i in range(10):\n",
    "    few_shot_indices = few_shot_indices + \\\n",
    "        random.sample(digit_groups_train[i], n_samples)\n",
    "\n",
    "# few_shot_indices = random.sample(all_img_indices, n_samples)\n",
    "\n",
    "sup_imgs_train = torch.utils.data.Subset(dataset1, few_shot_indices)\n",
    "\n",
    "sup_train_loader = torch.utils.data.DataLoader(\n",
    "    sup_imgs_train, **nn_train_kwargs)\n",
    "\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=1.0)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "\n",
    "for epoch in range(1, 5):\n",
    "    train(model, device, sup_train_loader,\n",
    "        optimizer, epoch)\n",
    "    #test(model, device, nn_test_loader)\n",
    "    scheduler.step()\n",
    "test(model, device, nn_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:02<00:00, 1085.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 1.983724\n",
      "Abduction accuracy:  0.5476666666666666\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 1.375738\n",
      "Abduction accuracy:  0.5476666666666666\n",
      "-- Test set: Average loss: 0.0165, Accuracy: 6179/10000 (62%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:02<00:00, 1097.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 1.154074\n",
      "Abduction accuracy:  0.839\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.687064\n",
      "Abduction accuracy:  0.839\n",
      "-- Test set: Average loss: 0.0066, Accuracy: 8846/10000 (88%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:02<00:00, 1092.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.383940\n",
      "Abduction accuracy:  0.97\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.261652\n",
      "Abduction accuracy:  0.97\n",
      "-- Test set: Average loss: 0.0031, Accuracy: 9461/10000 (95%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:02<00:00, 1085.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.143895\n",
      "Abduction accuracy:  0.9933333333333333\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.134408\n",
      "Abduction accuracy:  0.9933333333333333\n",
      "-- Test set: Average loss: 0.0026, Accuracy: 9537/10000 (95%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:02<00:00, 1106.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.149855\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.123469\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "-- Test set: Average loss: 0.0025, Accuracy: 9555/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:02<00:00, 1098.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.134329\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.097142\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "-- Test set: Average loss: 0.0024, Accuracy: 9569/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:02<00:00, 1107.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.139224\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.142271\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "-- Test set: Average loss: 0.0024, Accuracy: 9572/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:02<00:00, 1088.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.105255\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.103161\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "-- Test set: Average loss: 0.0024, Accuracy: 9575/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:02<00:00, 1054.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.133247\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.092401\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "-- Test set: Average loss: 0.0024, Accuracy: 9575/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:02<00:00, 1069.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.112440\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.079905\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "-- Test set: Average loss: 0.0024, Accuracy: 9576/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:02<00:00, 1072.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.105324\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.076962\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "-- Test set: Average loss: 0.0024, Accuracy: 9577/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:02<00:00, 1070.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.112345\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.103717\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "-- Test set: Average loss: 0.0024, Accuracy: 9577/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:02<00:00, 1065.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.167619\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.105299\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "-- Test set: Average loss: 0.0024, Accuracy: 9576/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:02<00:00, 1058.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.101125\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.084854\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "-- Test set: Average loss: 0.0024, Accuracy: 9577/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:02<00:00, 1044.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.057011\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.179679\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "-- Test set: Average loss: 0.0024, Accuracy: 9576/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:02<00:00, 1044.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.117506\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.157662\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "-- Test set: Average loss: 0.0024, Accuracy: 9576/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:03<00:00, 965.70it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.131000\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.119853\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "-- Test set: Average loss: 0.0024, Accuracy: 9576/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:02<00:00, 1071.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.120961\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.119883\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "-- Test set: Average loss: 0.0024, Accuracy: 9576/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:02<00:00, 1060.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.157537\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.101702\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "-- Test set: Average loss: 0.0024, Accuracy: 9576/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:02<00:00, 1064.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.088560\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.118095\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "-- Test set: Average loss: 0.0024, Accuracy: 9576/10000 (96%)\n"
     ]
    }
   ],
   "source": [
    "ABL_epochs = 20\n",
    "for epoch in range(ABL_epochs):\n",
    "    ABL_main(model, prolog, mnist_sum_data_train, optimizer=optimizer, scheduler=scheduler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
